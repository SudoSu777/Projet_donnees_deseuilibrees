---
title: "Projet Données Déséquilibrées"
author: ""
date: "23/12/2020"
output:
    rmdformats::readthedown:
    self_contained: no
    css: style.css
---



```{r Knitr_Global_Options, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE, cache = TRUE, autodep = TRUE, tidy = FALSE)
```



# Quelques algorithmes de ré-équilibrage

```{r}
library(ggplot2)
library(dplyr)
library(UBL)
library(cccd)
library(randomForest)
library(caret)
```

```{r}
n <- 2000
set.seed(1234)
X1 <- runif(n)
set.seed(5678)
X2 <- runif(n)
set.seed(9012)
R1 <- X1<=0.25
R2 <- (X1>0.25 & X2>=0.75)
R3 <- (X1>0.25 & X2<0.75)
Y <- rep(0,n)
Y[R1] <- rbinom(sum(R1),1,0.75)
Y[R2] <- rbinom(sum(R2),1,0.75)
Y[R3] <- rbinom(sum(R3),1,0.25)
df1 <- data.frame(X1,X2,Y)
df1$Y <- factor(df1$Y)
indDY1 <- which(df1$Y==1)
df1.1 <- df1[-indDY1[1:650],]
df1.2 <- df1.1[sample(nrow(df1.1),1000),]
df <- df1.2[sample(nrow(df1.2),100),]
rownames(df) <- NULL
p1 <- ggplot(df)+aes(x=X1,y=X2,color=Y)+geom_point()
p1
```
```{r}
summary(df$Y)
```
On constate un déséquilibre très fort.

### **1. On commence par faire du oversampling avec la fonction RandOverClassif.**

L'Oversampling est une méthode dite de ré-échantillonnage qui consiste à faire du sur-échantillonnage c'est à dire dupliquer des individus de la classe minoritaire pour faire face aux problèmes de données déséquilibrées.

**a. Effectuer le ré-échantillonnage et expliquer.**

 Ici, nous utiliserons la fonction **RandOverClassif** du package **UBL** pour dupliquer aléatoirement des observations dans la classe minoritaire.

La fonction renvoie une base de données avec le nouvel ensemble de données résultant de l'application de la stratégie de sur-échantillonnage aléatoire.
```{r}
over_df1 <- RandOverClassif(Y~., df, "balance")

```
Ici cette fonction fait un équilibrage total de la base en mettant toutes nos deux classes au même niveau.

```{r}
summary(over_df1$Y)
```
```{r}
cat("Base initial :",sum(duplicated(df)),"\n")
cat("Base après sur-échantillonnage :",sum(duplicated(over_df1)),"\n")

```
On constate qu'il y a plusieurs individus de la base qui ont été dupliqués.

**b. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1.**
```{r}
df$Y=as.factor(df$Y)
```


```{r}
C.perc = list("1"= 3) 
over_df2 <- RandOverClassif(Y~., df, C.perc)
```

```{r}
summary(over_df2$Y)
```

## **2. On s’intéresse maintenant à l’algorithme SMOTE**

**SMOTE** est une technique de sur-échantillonnage permettant de résoudre le problème de données deséquilibrées en générant des individus synthétiques.

**a. Exécuter la fonction SmoteClassif avec k=3 et les les paramètres par défaut**

Cette fonction gère les problèmes de classification déséquilibrées à l'aide de la méthode **SMOTE**. Elle prend en paramètre le nombre de voisins à considérer pour un individu tiré au hasard, et utilise par défaut la distance euclidienne pour le calcul des **k-voisins** les plus proches.

```{r}
smote_df1=SmoteClassif(Y~., df, k = 3)
```

```{r}
summary(smote_df1$Y)

```
on constate  un équilibre total des classes, avec une légère diminution de la classe **0** et une légère augmentation de la classe **1**.

```{r}
cat("Base initial :",sum(duplicated(df)),"\n")
cat("Base après sur-échantillonnage :",sum(duplicated(smote_df1)),"\n")
```

On voit là que la méthode **SMOTE** ne duplique aucun individu mais plutôt crée des individus synthétiques (imaginaire).

**b. Visualiser les observations smote.**

```{r}
p1 <- ggplot(smote_df1)+aes(x=X1,y=X2,color=Y)+geom_point()
p1
```

**c. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1.**

```{r}
 C.perc = list("1"=3)
smote_df2=SmoteClassif(Y~., df, k = 3, C.perc)

```

```{r}
summary(smote_df2$Y)
```

## **3. On souhaite maintenant ré-équilibrer par random undersampling. Utiliser la fonction RandUnderClassif pour effectuer un tel ré-équilibrage. Ici encore on pourra faire varier les paramètres.**

**Undersampling**  est une technique de ré-échantillonnage qui consiste à faire un sous échantillonnage de la classe majoritaire en supprimant des individus tirés au hasard.

Ici nous utiliserons la fonction **RandUnderClassif** du package **UBL**.

```{r}
under_df1=RandUnderClassif(Y~., df)
summary(under_df1$Y)

```
Ici la fonction fait par défaut un équilibrage total en ramenant les deux classes au même nombre d'individu.
```{r}
under_df2=RandUnderClassif(Y~., df,C.perc = list("0"=0.5))
summary(under_df2$Y)

```
Ici nous optons pour un équilibrage moins souple où la classe majoritaire est égale à 2 fois la classe minoritaire.

## **4. On passe maintenant à l’algorithme Tomek.**

**a. sans utiliser la fonction TomekClassif identifier les paires d’observations qui ont un lien de Tomek. On pourra utiliser la fonction nng du package cccd.**

**Tomek**  est une approche qui consiste à supprimer les observations de la classe majoritaire qui se trouvent proches d’observations de la classe minoritaire de sorte à éliminer les observations de la classe **k** qui se trouve entre les observations de la classe **d** en vue de mieux détecter chacune des observations des deux classes.


On dira que deux individus x,y ont un lien de Tomek si pour toute autre observation z, d(x,y)<d(x,z) et d(x,y)<d(y,z), où d est une distance et x est de la classe majoritaire et y de la classe minoritaire.

```{r}
liens_tommek <- nng(df,k=5)
```



**b. Retrouver ces paires à l’aide de la fonction Tomek LinK**
```{r}
tomek_liens=TomekClassif(Y~., df)
                          
```

```{r}
dim(df[tomek_liens[[2]],])
```

On observe 18 individus qui ont un lien de Tomek.
```{r}
cat("Individus ayant un lien de Tomek :","\n")
df[tomek_liens[[2]],]
```
**c. Visualiser les observations supprimées. On prendra soin d’expliquer l’option rem de TomekClassif.**

**rem** est une chaîne de caractères indiquant si les deux exemples formant le lien Tomek doivent être supprimés ou si seul l'exemple de la classe la plus large doit être supprimé. Dans le premier cas, ce paramètre doit être reglé sur "both" et dans le second cas sur "maj".

```{r}
p1 <- ggplot(df[tomek_liens[[2]],])+aes(x=X1,y=X2,color=Y)+geom_point()
p1
```


# Comparaison de méthodes de ré-équilibrage


```{r}
n <- 2000
set.seed(12345)
X1 <- runif(n)
set.seed(5678)
X2 <- runif(n)
set.seed(9012)
R1 <- X1<=0.25
R2 <- (X1>0.25 & X2>=0.75)
R3 <- (X1>0.25 & X2<0.75)
Y <- rep(0,n)
Y[R1] <- rbinom(sum(R1),1,0.75)
Y[R2] <- rbinom(sum(R2),1,0.75)
Y[R3] <- rbinom(sum(R3),1,0.25)
df1 <- data.frame(X1,X2,Y)
df1$Y <- factor(df1$Y)
indDY1 <- which(df1$Y==1)
df2 <- df1[-indDY1[1:400],]
df3 <- df1[-indDY1[1:700],]
df1 <- df1[sample(nrow(df1),1000),]
df2 <- df2[sample(nrow(df2),1000),]
df3 <- df3[sample(nrow(df3),1000),]
```

## **1. Comparer la distribution de Y pour ces trois jeux de données et visualiser les observations.**

```{r}
cat("Distribution des classes de df1","\n")
summary(df1$Y)
cat("\n")
cat("Distribution des classes de df2","\n")
summary(df2$Y)
cat("\n")
cat("Distribution des classes de df3","\n")
summary(df3$Y)
cat("\n")
cat("Jeu de données 1 Vs 2","\n")
table(df1$Y,df2$Y)
cat("\n")

cat("Jeu de données 1 vs 3","\n")
table(df1$Y,df3$Y)
cat("\n")

cat("Jeu de données 2 vs 3","\n")
table(df2$Y,df3$Y)
cat("\n")


```
```{r}
p1 <- ggplot(df1)+aes(x=X1,y=X2,color=Y)+geom_point()+ggtitle("Distribution 1")
p2 <- ggplot(df2)+aes(x=X1,y=X2,color=Y)+geom_point()+ggtitle("Distribution 2")
p3 <- ggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point()+ggtitle("Distribution 3")

```

```{r}
p1
```
```{r}
p2
```
```{r}
p3
```

## **2. On sépare ces 3 échantillons en un échantillon d’apprentissage et un échantillon test.**

```{r}
set.seed(123)
a1 <- createDataPartition(1:nrow(df1),p=2/3)
a2 <- createDataPartition(1:nrow(df2),p=2/3)
a3 <- createDataPartition(1:nrow(df3),p=2/3)
train1 <- df1[a1$Resample1,]
train2 <- df2[a2$Resample1,]
train3 <- df3[a3$Resample1,]
test1 <- df1[-a1$Resample1,]
test2 <- df2[-a2$Resample1,]
test3 <- df3[-a3$Resample1,]
```

**Echantillon 1**
```{r}
rf1=randomForest(Y~ .,data=train1)
y_pred1=predict(rf1,test1[,-3])
confusionMatrix(test1$Y,y_pred1,mode="everything")

```

**Echantillon 2**
```{r}
rf2=randomForest(Y~ .,data=train2)
y_pred2=predict(rf2,test2[,-3])
confusionMatrix(test2$Y,y_pred2,mode="everything")

```

**Echantillon 3**
```{r}
rf3=randomForest(Y~ .,data=train3)
y_pred3=predict(rf3,test3[,-3])
confusionMatrix(test3$Y,y_pred3,mode="everything")

```

Les échantillons 1,2,3 présentent des distributions différentes. Nous pouvons remarquer que pour les echantillons ayant un fort désequilibre de classes, nous obtenons de très belle performance en **Accuracy**,**F1** et **Balanced Accuracy** notamment pour les échantillons **Echantillon2** et **Echantillon3**.
Ce résultat se traduit par le fait que pour les échantillons 2 et 3 le classifieur ajusté (RandomForest) semble prédire plus les individus présents dans la classe majoritaire qui est la classe **0** qui biaise les résultats.

## **3. On considère uniquement l’échantillon df3. Refaire l’analyse précédente en utilisant des techniques de ré-échantillonnage.**


- RandOversampling

```{r}
over_df3 <- RandOverClassif(Y~., df3)
set.seed(123)

a3 <- createDataPartition(1:nrow(over_df3),p=2/3)

train3 <- over_df3[a3$Resample1,]

test3 <- over_df3[-a3$Resample1,]
rf3=randomForest(Y~ .,data=train3)
y_pred3=predict(rf3,test3[,-3])
confusionMatrix(test3$Y,y_pred3,mode="everything")
```

- RandUndersampling

```{r}
under_df3=RandUnderClassif(Y~., df3)
set.seed(123)

a3 <- createDataPartition(1:nrow(under_df3),p=2/3)

train3 <- under_df3[a3$Resample1,]

test3 <- under_df3[-a3$Resample1,]
rf3=randomForest(Y~ .,data=train3)
y_pred3=predict(rf3,test3[,-3])
confusionMatrix(test3$Y,y_pred3,mode="everything")
```

- SMOTE

```{r}
smote_df3=SmoteClassif(Y~., df3)
set.seed(123)

a3 <- createDataPartition(1:nrow(smote_df3),p=2/3)

train3 <- smote_df3[a3$Resample1,]

test3 <- smote_df3[-a3$Resample1,]
rf3=randomForest(Y~ .,data=train3)
y_pred3=predict(rf3,test3[,-3])
confusionMatrix(test3$Y,y_pred3,mode="everything")
```

- Tomek
```{r}
tomek_liens=TomekClassif(Y~., df3)
df_tomek=df3[-tomek_liens[[2]],]

set.seed(123)

a3 <- createDataPartition(1:nrow(df_tomek),p=2/3)

train3 <- df_tomek[a3$Resample1,]

test3 <- df_tomek[-a3$Resample1,]
rf3=randomForest(Y~ .,data=train3)
y_pred3=predict(rf3,test3[,-3])
confusionMatrix(test3$Y,y_pred3,mode="everything")
```
Les résultats obtenus montrent de bonne performance des méthodes **Oversampling** et **Tomek** par rapport à **SMOTE** et **Undersampling**.
En effet,comparées aux résultats obtenus à la question précedente, les méthodes de ré-équilibrage nous ont permis d'accroitre nos performances.
Nous pouvons au vu de ces résultats affirmer que les méthodes **Oversampling** et **Tomek** semblent plus efficaces pour notre étude car elles nous ont permis de rélever les performances de notre classifieur (RandomForest) .

On peut également constater que les méthodes de ré-équilibrage sont intéressantes pour accroitre le pouvoir prédicteur de nos différentes classes.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# IV
```{r}
library(tidyverse)
library(e1071)
library(caret)
library(rpart)
library(glmnet)
library(ranger)
library(gbm)
library(doParallel)
library(foreach)
library(klaR)
library(kernlab)
```



```{r}
data_path="F:/document/IDSI COURS/IDSI/MASTER2/DonneesDes/TP/ad.data"
```

```{r}
ad.data <- read.table(data_path,header=FALSE,sep=",",dec=".",na.strings = "?",strip.white = TRUE)
dim(ad.data)
```
On est en grande dimension car le nombre de variables est l'argement supérieur au nombre de lignes.


```{r}
names(ad.data)[ncol(ad.data)] <- "Y"
ad.data$Y <- as.factor(ad.data$Y)
summary(ad.data$Y)
```


```{r}
sum(is.na(ad.data))
```
On remarque que :
- 920 lignes
- 4 colonnes
ont au moins une valeur manquante.

```{r}
apply(is.na(ad.data),1,any) %>% sum()
```


```{r}
var.na <- apply(is.na(ad.data),2,any)
```

Colonnes ayant des valeurs manquantes
```{r}
names(ad.data)[var.na]
```


Vérifions le taux de nan dans ces variables
```{r}
for (i in names(ad.data)[var.na] )
  {
  cat("Variable ",i ,"contient :",100*(apply(is.na(ad.data[i]),1,any) %>% sum())/nrow(ad.data), "% valeurs manquantes","\n")
  }
cat("On a au total :",sum(var.na),"Variables contenant des nan soit ", 100*(apply(is.na(ad.data[var.na]),1,any) %>% sum())/nrow(ad.data),"% d'individus de la base")

```
Ces variables étant toutes de type numérique, nous décidons de remplacer les nan par des valeurs numériques.

**variable V1**
```{r}
summary(ad.data['V1'])
```

On peut constater que cette variables contiennent des valeurs aberrantes, la moyenne ou la médian ne serait pas des indicateurs pertinents pour remplacer les valeurs nan.On va donc effectuer un remplacement des nan par interpolation linéaire.


```{r}
 
ind<-1:nrow(ad.data)
liste.na<-ind[is.na(ad.data['V1'])]
nona<-ind[!is.na(ad.data['V1'])]
 
 
ad.data[liste.na,1] <- approx(x=nona,y=ad.data[nona,1],method="linear",xout=liste.na)$y

```


**variable V2**
```{r}
summary(ad.data['V2'])

```
On constate également la présence  de valeurs aberrantes dans cette varaible.On utilise l'interpolation linéaire pour remplacer les nan.

```{r}

ind<-1:nrow(ad.data)
liste.na<-ind[is.na(ad.data['V2'])]
nona<-ind[!is.na(ad.data['V2'])]
 
 
ad.data[liste.na,2] <- approx(x=nona,y=ad.data[nona,2],method="linear",xout=liste.na)$y
```

**Variable V3**
```{r}
summary(ad.data['V3'])
```

On remplacera les nan par interpolation linéaire.

```{r}

ind<-1:nrow(ad.data)
liste.na<-ind[is.na(ad.data['V3'])]
nona<-ind[!is.na(ad.data['V3'])]
 
 
ad.data[liste.na,3] <- approx(x=nona,y=ad.data[nona,3],method="linear",xout=liste.na)$y
```

**Variable V4**

```{r}
summary(ad.data['V4'])
```
```{r}

ind<-1:nrow(ad.data)
liste.na<-ind[is.na(ad.data['V4'])]
nona<-ind[!is.na(ad.data['V4'])]
 
 
ad.data[liste.na,4] <- approx(x=nona,y=ad.data[nona,4],method="linear",xout=liste.na)$y
```

Vérifions si les nan ont été remplacées

```{r}
cat("On a au total :",sum(is.na(ad.data)),"Variables contenant des nan soit ", 100*(apply(is.na(ad.data[var.na]),1,any) %>% sum())/nrow(ad.data),"% d'individus de la base")
```


```{r}
ad.data1=as.data.frame(ad.data)
```

On construit la matrice des X et le vecteur des Y qui sont nécessaires pour certaines fonctions comme glmnet.

```{r}
X.ad <- model.matrix(Y~.,data=ad.data1)[,-1]
Y.ad <- ad.data1$Y
```
on transforme la variable cible en 0-1 pour utiliser gbm.

```{r}
ad.data2 <- ad.data1 %>% mutate(Y=recode(Y,"ad."=0,"nonad."=1))
```

On souhaite comparer les algorithmes présentés précédemment. On commence tout d’abord par représenter un algorithme par une fonction R qui admettra en entrée un jeu de données et renverra une unique prévision pour de nouveaux individus. On illustre ces fonctions pour prédire ce nouvel individu.

```{r}
newX <- ad.data1[1000,]
newX.X <- matrix(X.ad[1000,],nrow=1)
```
On stockera les prévisions dans l’objet suivant:

```{r}
prev <- tibble(algo=c("SVM","arbre","ridge","lasso","foret","ada","logit"),prev=0)
```


#SVM à noyau Gaussien par validation croisée 4 blocs

```{r}
prev.svm <- function(df,newX){
  C <- c(0.01,1,10)
  sigma <- c(0.1,1,3)
  gr <- expand.grid(C=C,sigma=sigma)
  ctrl <- trainControl(method="cv",number=4)
  cl <- makePSOCKcluster(3)
  registerDoParallel(cl)
  res.svm <- train(Y~.,data=df,method="svmRadial",trControl=ctrl,
               tuneGrid=gr,prob.model=TRUE)
  stopCluster(cl)
  predict(res.svm,newX,type="prob")[2]
}
prev[1,2] <- prev.svm(ad.data1,newX)
```


#Arbre de classification

```{r}
prev.arbre <- function(df,newX){
  arbre <- rpart(Y~.,data=df,cp=1e-8,minsplit=2)
  cp_opt <- arbre$cptable %>% as.data.frame() %>% filter(xerror==min(xerror)) %>% 
dplyr::select(CP) %>% slice(1) %>% as.numeric()
  arbre.opt <- prune(arbre,cp=cp_opt)
  predict(arbre,newdata=newX,type="prob")[,2]
}
prev[2,2] <- prev.arbre(ad.data1,newX)
```


#Lasso et Ridge

```{r}
prev.ridge <- function(df.X,df.Y,newX){
  ridge <- cv.glmnet(df.X,df.Y,family="binomial",alpha=0)
  as.vector(predict(ridge,newx = newX,type="response"))
}
prev.lasso <- function(df.X,df.Y,newX){
  lasso <- cv.glmnet(df.X,df.Y,family="binomial",alpha=1)
  as.vector(predict(lasso,newx = newX,type="response"))
}
prev[3,2] <- prev.ridge(X.ad,Y.ad,newX.X)
prev[4,2] <- prev.lasso(X.ad,Y.ad,newX.X)
```

#Forêt aléatoire

```{r}
prev.foret <- function(df,newX){
  foret <- ranger(Y~.,data=df,probability=TRUE)
  predict(foret,data=newX,type="response")$predictions[,2]
}
prev[5,2] <- prev.foret(ad.data1,newX)
```


#Adaboost et Logitboost

```{r}
prev.ada <- function(df,newX){
  ada <- gbm(Y~.,data=df,distribution="adaboost",interaction.depth=2,
         bag.fraction=1,cv.folds = 5,n.trees=500)
  nb.it <- gbm.perf(ada,plot.it=FALSE)
  predict(ada,newdata=newX,n.trees=nb.it,type="response")
}

prev.logit <- function(df,newX){
  logit <- gbm(Y~.,data=df,distribution="bernoulli",interaction.depth=2,
           bag.fraction=1,cv.folds = 5,n.trees=500)
  nb.it <- gbm.perf(logit,plot.it=FALSE)
  predict(logit,newdata=newX,n.trees=nb.it,type="response")
}    

prev[6,2] <- prev.ada(ad.data2,newX)
prev[7,2] <- prev.logit(ad.data2,newX)
```


```{r}
prev
```




#Utilisons SVM et Decision Tree
```{r}
cl <- makePSOCKcluster(3)
  registerDoParallel(cl)

control <- trainControl(method='cv', number=10, savePredictions=TRUE)

# Support Vector Machines (SVM) 
set.seed(101)
fit.svm <- train(Y~., data=ad.data2, method='svmRadial',
                 preProc=c('center', 'scale'), trControl=control)

stopCluster(cl)
```

```{r}
cl <- makePSOCKcluster(3)
  registerDoParallel(cl)

control <- trainControl(method='cv', number=10, savePredictions=TRUE,classProbs = T)

set.seed(101)
fit.rpart <- train(Y~., data=ad.data2, method='rpart',
                 preProc=c('center', 'scale'), trControl=control)

stopCluster(cl)
```




```{r }
get_accuracy=function(df,metric,model){
  cl <- makePSOCKcluster(3)
  registerDoParallel(cl)

control <- trainControl(method='cv', number=10, savePredictions=TRUE)

set.seed(101)
fit.model <- train(Y~., data=df, method=model, metric=metric,
                 preProc=c('center', 'scale'), trControl=control)

stopCluster(cl)
fit.model$results
}

```

```{r }
get_auc=function(df,metric,model){
  cl <- makePSOCKcluster(3)
  registerDoParallel(cl)

control <- trainControl(method='cv', number=10, savePredictions=TRUE,classProbs = TRUE,summaryFunction = twoClassSummary)

set.seed(101)
fit.model <- train(Y~., data=df, method=model, metric=metric,
                  trControl=control)

stopCluster(cl)
fit.model$results
}
```

```{r}
data=ad.data2  %>% 
  mutate(Y = factor(Y, 
          labels = make.names(levels(Y))))
```


```{r}
ROC='ROC'
model='svmRadial'
auc_svm=get_auc(data,ROC,model)

```

```{r }
metric="Accuracy"
model="svmRadial"
ac_svm=get_accuracy(data,metric,model)
```


#Utilisons rpart
```{r}
metric='ROC'
model='rpart'
auc_rpart=get_auc(data,metric,model)
```


```{r}
metric='Accuracy'
ac_rpart=get_accuracy(data,metric,model='rpart')
```
```{r}
metrics=data.frame(rpart=c(mean(auc_rpart$ROC),1-mean(ac_rpart$Accuracy)),SVM=c(mean(auc_svm$ROC),1-mean(ac_svm$Accuracy)),row.names = c('ROC','Classification erros'))
```
```{r}
print(metrics)

```

Nous avons choisi deux classifieurs qui sont: **SVM** et **Les arbres de décision**, nous pouvons remarquer que le classifieur SVM avec un taux d'erreur faible semble être plus performant que celui de l'arbre de décision.

Courbe ROC de nos deux classifieurs
```{r}
#SVM
library(MLeval)
```

**ROC Curve**
```{r}
res=evalm(list(fit.svm,fit.rpart),title="Comparaison de modèles ",gnames=c("SVM","rpart"))
```


Le résultat obtenu plus haut avec **l'erreur de classification** et la métrique **ROC** se justifie ici par la courbe ROC où nous observons une forte pente pour la courbe de la **SVM** avec un **AUC** de **0.97**, ce qui est différent de celui obtenu avec le classifieur **arbre de décision**.
Par conséquent,si nous devions choisir un classifieur pour notre problème ,nous nous tournerons vers la **SVM**.


# Exercice 8.2 (Choix d’un algorithme de ré-équilibrage par validation croisée)

```{r}
library(dplyr)
library(UBL)
library(cccd)
library(glmnet)
library(DMwR)
```

Pour cet exercice, nous parcourirons les méthodes de ré-équlibrage suivants :
**Oversampling** ,**Undersampling** et **SMOTE** auxquelles seront ajustés les classifieurs suivants : **Logistique ridge**, **Logistique Lasso** et **SVM**. Pour réaliser celà nous nous servirons des librairies **Caret** et **DMwR**. 
Puisqu'il est question de faire du ré-équilibrage par validation croisée alors les packages **caret** et **DMwR** serviront à appliquer les méthodes de ré_équilibrage à chaque **fold** sur le jeu de données d'apprentissage.


# Méthode de ré-équilibrage et ajustement des classifieurs
```{r}
data=data.frame(ad.data2)
#-------------------------------------------------------------------underrsampled_ridge_lasso__rpart


#Logistic Regression (Ridge)

myControl <- trainControl(method = "cv", number  = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     ## new option here:
                     sampling = "down")
myGrid <- expand.grid(
                       alpha = 0,
                       lambda =0.1
                      )

# Fit the model
set.seed(42)
under.ridge <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = myGrid, trControl = myControl)
#-----------------------------------------rpart


set.seed(101)
under.rpart <- train(Y~., data=data, method='rpart',metric='ROC'
                 , trControl=myControl)
#-----------------------------------------Lasso
under.lasso <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = expand.grid(
                       alpha = 1,
                       lambda =0.1
                      ), trControl = myControl)

```

```{r}
#-------------------------------------------------------------------oversampled_ridge_lasso_rpart

#Logistic Regression (Ridge)

myControl <- trainControl(method = "cv", number  = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     ## new option here:
                     sampling = "up")
myGrid <- expand.grid(
                       alpha = 0,
                       lambda =0.1
                      )

# Fit the model
set.seed(42)
over.ridge <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = myGrid, trControl = myControl)
#-----------------------------------------rpart

control <- trainControl(method='cv', number=10, savePredictions=TRUE,classProbs = T,summaryFunction = twoClassSummary)

set.seed(101)
over.rpart <- train(Y~., data=data, method='svmRadial',metric='ROC'
                 , trControl=myControl)
#-----------------------------------------Lasso
over.lasso <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = expand.grid(
                       alpha = 1,
                       lambda =0.1
                      ), trControl = myControl)

```

```{r}
#-------------------------------------------------------------------smote_ridge_lasso_rpart

#Logistic Regression (Ridge)

myControl <- trainControl(method = "cv", number  = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     ## new option here:
                     sampling = "smote")
myGrid <- expand.grid(
                       alpha = 0,
                       lambda =0.1
                      )

# Fit the model
set.seed(42)
smote.ridge <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = myGrid, trControl = myControl)
#-----------------------------------------rpart

control <- trainControl(method='cv', number=10, savePredictions=TRUE,classProbs = T,summaryFunction = twoClassSummary)

set.seed(101)
smote.rpart <- train(Y~., data=data, method='svmRadial',metric='ROC'
                 , trControl=myControl)
#-----------------------------------------Lasso
smote.lasso <- train(Y ~., data = data, method = "glmnet",metric='ROC',
               tuneGrid = expand.grid(
                       alpha = 0,
                       lambda =0.1
                      ), trControl = myControl)

```


```{r}
under_data=data.frame(rpart=c(mean(under.rpart$results$ROC),mean(under.rpart$results$Sens),mean(under.rpart$results$Spec)),lasso=c(mean(under.lasso$results$ROC),mean(under.lasso$results$Sens),mean(under.lasso$results$Spec)),ridge=c(mean(under.ridge$results$ROC),mean(under.ridge$results$Sens),mean(under.ridge$results$Spec)),row.names = c("ROC","Specificity","Sensitivity"))

#----------------------------------------------------
over_data=data.frame(rpart=c(mean(over.rpart$results$ROC),mean(over.rpart$results$Sens),mean(over.rpart$results$Spec)),lasso=c(mean(over.lasso$results$ROC),mean(over.lasso$results$Sens),mean(over.lasso$results$Spec)),ridge=c(mean(over.ridge$results$ROC),mean(over.ridge$results$Sens),mean(over.ridge$results$Spec)),row.names = c("ROC","Specificity","Sensitivity"))

#------------------------------------------------
smote_data=data.frame(rpart=c(mean(smote.rpart$results$ROC),mean(smote.rpart$results$Sens),mean(smote.rpart$results$Spec)),lasso=c(mean(smote.lasso$results$ROC),mean(smote.lasso$results$Sens),mean(smote.lasso$results$Spec)),ridge=c(mean(smote.ridge$results$ROC),mean(smote.ridge$results$Sens),mean(smote.ridge$results$Spec)),row.names = c("ROC","Specificity","Sensitivity"))
```

**UnderSampling**
```{r}
under_data

```

**Oversampling**
```{r}
over_data
```

**SMOTE**
```{r}
smote_data
```

A travers les métriques obtenu ci-dessus, la méthode **smote** donne de meilleures performances.Parmi les classifieurs ajustés, la **logistique ridge** présente les meilleures scores.


